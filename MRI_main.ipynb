{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHhLAJdzxZivBRp3+ubAAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konstvv/DL_MRI_reconstriction/blob/main/MRI_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO list:\n",
        "1. Find the dataset (High-quality MRI images)\n",
        "* small dataset - https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\n",
        "* large dataset - https://fastmri.med.nyu.edu/ (need to request the access)\n",
        "* Something else?\n",
        "\n",
        "2. Preprocess the data\n",
        "\n",
        " 2.1 HQ images -> make them noisy\n",
        "\n",
        " * White noise - every pixel would get a SMALL random increment\n",
        " * Gaussian blur - https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter.html\n",
        "\n",
        " 2.2 (maybe optional, depends on dataset size) Data augmentation\n",
        " * Rotation (saving the original size)\n",
        " * Zoom\n",
        " * Mirror\n",
        "\n",
        "Training sample - pair of images (clean and corrupted images: label and sample)\n",
        "\n",
        "3. Creation of the model\n",
        "\n",
        "  * Supervised learning paradigm\n",
        "  * Baseline model - CNN architecture (dims of input and output are the same)\n",
        "  * Compare the output to the orginal image:\n",
        "      * PSNR(Peak Signal-to-Noise Ratio)\n",
        "      * SSIM\n",
        "  * Overview of the models: https://arxiv.org/pdf/1912.13171.pdf\n",
        "  * Brain MRI model: https://aapm.onlinelibrary.wiley.com/doi/epdf/10.1002/acm2.13758\n",
        "\n",
        "  * General overview: https://towardsai.net/p/deep-learning/image-de-noising-using-deep-learning\n",
        "\n",
        "4. Tuning and testing\n"
      ],
      "metadata": {
        "id": "VjMl2ebMw1ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 1\n",
        "# Dataset import\n",
        "# Upload it to Google Drive\n",
        "# Fetch the data Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIAFE8VZyDUg",
        "outputId": "a8de9f30-96c6-4ff2-f30f-f8461557b531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filepath = '/content/drive/MyDrive'\n",
        "for file in os.listdir(filepath):\n",
        "  print(file)\n",
        "  # Code that imports all the images\n",
        "  # Two options of output:\n",
        "  ## list with all the images (fast, a lot of RAM) -> preferable method\n",
        "  ## create a separate folder with all the images + function to preprocess images in batches"
      ],
      "metadata": {
        "id": "mm3C9H_b48fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 2\n",
        "# Datset preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "pic1 = plt.imread('cat.jpeg')\n",
        "pic2 = plt.imread('parrot.jpg')\n",
        "plt.imshow(pic1)\n",
        "plt.figure()\n",
        "plt.imshow(pic2)"
      ],
      "metadata": {
        "id": "f9B2BVVW5W_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomContrast, RandomZoom\n",
        "\n",
        "class NoiseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, noise_type='gauss'):\n",
        "    super(NoiseLayer, self).__init__()\n",
        "    self.noise_type = noise_type\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    pass\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if self.noise_type == \"gauss\":\n",
        "        mean = 0\n",
        "        var = 0.1\n",
        "        gauss = np.random.normal(mean, var**0.5, inputs.shape)\n",
        "        gauss = gauss.reshape(*inputs.shape)\n",
        "        noisy = inputs + gauss\n",
        "        return noisy\n",
        "    elif self.noise_type == \"poisson\":\n",
        "        peak = 5\n",
        "        noisy = np.random.poisson(inputs * peak) / float(peak)\n",
        "        return noisy\n",
        "\n",
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal_and_vertical\"),\n",
        "    RandomZoom(0.1, fill_mode='nearest'),\n",
        "    RandomRotation(0.2, fill_mode='nearest'),\n",
        "    RandomContrast(0.3),\n",
        "    NoiseLayer(noise_type='poisson')])\n",
        "\n",
        "images = [pic1, pic2]\n",
        "images = [cv2.resize(img, (800, 800)) for img in images]\n",
        "print(images[0].shape, images[1].shape)\n",
        "images = np.stack(images, axis=0)/255.\n",
        "print(images.shape)\n",
        "\n",
        "for i in range(9):\n",
        "  augmented_images = data_augmentation(images)\n",
        "  noisy_images = tf.clip_by_value(augmented_images, 0., 1.)\n",
        "  for j in range(noisy_images.shape[0]):\n",
        "    plt.figure()\n",
        "    ax = plt.subplot(noisy_images.shape[0], noisy_images.shape[0], j + 1)\n",
        "    plt.imshow(noisy_images[j])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "_u8RroSOdZgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 3\n",
        "# Model architechture\n",
        "# Keras to create baseline CNN model"
      ],
      "metadata": {
        "id": "XHGojx4P5_gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 4\n",
        "# Evaluation and testing"
      ],
      "metadata": {
        "id": "jJTS9j486Ogv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}